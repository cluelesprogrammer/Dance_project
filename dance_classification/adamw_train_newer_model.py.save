import torch
import torch
import numpy as np
import time
import argparse
import wandb
import yaml
from tqdm import tqdm
from loss import paf_loss, conf_loss
from data_loader import lets_dance_loaders, coco_mpii_loaders, concatenated_dataloaders
from models_loader import get_model
from opt_sch_loaders import get_cyclic_sch, get_warm_restarts_sch, MultipleOptimizer, MultipleScheduler
from datasets.dataset import GroundTruthGenerator as G
import torch.optim as optim
import math
from train import train_epoch, val_epoch

device = torch.device('cuda:2')

min_nuns_val_loss, min_runs_paf_val_loss, min_runs_conf_val_loss = math.inf, math.inf, math.inf


def adamw_find(config=None):
	with wandb.init(project='the_adamw_experiments', entity='sbaral',config=config):
		config = wandb.config
		dataloader = coco_mpii_loaders(384, config['batch_size'], 3, 2, 1, 1)
		train_features = bool(config['train_features'])
		model = get_model('new', train_features, final_activations=(config['paf_activation'], config['conf_activation'])).to(device)
		features_param = model.features_extractor.parameters()
		opt_mod = optim.AdamW(model.parameters())
		if (train_features):
			opt_ftrs = optim.SGD(features_param, lr=config['features_lr'], momentum=0.95)
			sch_ftrs = get_cyclic_sch(opt_ftrs, config['features_lr']/10, config['features_lr'], int(len(dataloader['train'])/10), len(dataloader['train']))
			optimizers = MultipleOptimizer(opt_ftrs, opt_mod)
			schedulers = MultipleScheduler(opt_ftrs)
		else:
			optimizers = MultipleOptimizer(optim.AdamW(model.parameters()))
			schedulers = MultipleScheduler(None)
		print('after activation')
		min_val_loss, min_paf_val_loss, min_conf_val_loss = math.inf, math.inf, math.inf
		wandb.watch(model)
		for epoch in range(config['tot_epochs']):
			train_epoch(model, optimizers, schedulers, dataloader['train'], device, epoch)
			epoch_val_loss, epoch_paf_val_loss, epoch_conf_val_loss = val_epoch(model, dataloader['val'], device, epoch, 'coco')
			if ((epoch_val_loss < min_val_loss) and (epoch_paf_val_loss < min_paf_val_loss) and (epoch_conf_val_loss < min_conf_val_loss)):
				min_val_loss, min_paf_val_loss, min_conf_val_loss = epoch_val_loss, epoch_paf_val_loss, epoch_conf_val_loss
			else:
				break
				print('did not cause lower val_loss and paf_loss and conf_val loss')
			dict_to_save = {}
			dict_to_save['model_state_dict'] = model.state_dict()
			"""
			for i, opt in enumerate(optimizers):
				dict_to_save['opt_{}_state_dict'.format(i)] = opt.state_dict()
			for i, sch in enumerate(schedulers):
				if (sch):
					dict_to_save['sch_{}_state_dict'.format(i)] = sch.state_dict()
			"""
			saved_file_path = './models/adamw/admaw_activations_{}_{}_ftrs_lr_{}_epoch_{}.pt'.format(config['paf_activation'], config['conf_activation'], config['features_lr'], epoch)
			torch.save(dict_to_save, saved_file_path)
			print(saved_file_path)
		wandb.log({'min_val_loss': min_val_loss})
		return min_val_loss

"""
c = yaml.full_load(open('adam_ftrs_lr.yaml'))
sweep_id = wandb.sweep(c, project='the_adamw_experiments')
wandb.agent(sweep_id, adamw_find)
"""
config = {'batch_size': 64, 'conf_activation':'relu', 'paf_activation': 'tanh', 'tot_epochs':10, 'features_lr':1e-4, 'train_features':1}
adamw_find(config)
